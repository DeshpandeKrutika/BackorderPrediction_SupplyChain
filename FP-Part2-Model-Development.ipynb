{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Model Development\n",
    "\n",
    "In this part, we develop three unique pipelines for predicting backorder. We use the smart sample from Part I to fit and evaluate these pipelines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload the smart sample here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload your smart sampling from local file \n",
    "# ----------------------------------\n",
    "\n",
    "import joblib\n",
    "sampled_X, sampled_y= joblib.load('sampled_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>national_inv</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>in_transit_qty</th>\n",
       "      <th>forecast_3_month</th>\n",
       "      <th>forecast_6_month</th>\n",
       "      <th>forecast_9_month</th>\n",
       "      <th>sales_1_month</th>\n",
       "      <th>sales_3_month</th>\n",
       "      <th>sales_6_month</th>\n",
       "      <th>sales_9_month</th>\n",
       "      <th>min_bank</th>\n",
       "      <th>potential_issue</th>\n",
       "      <th>pieces_past_due</th>\n",
       "      <th>local_bo_qty</th>\n",
       "      <th>deck_risk</th>\n",
       "      <th>oe_constraint</th>\n",
       "      <th>ppap_risk</th>\n",
       "      <th>stop_auto_buy</th>\n",
       "      <th>rev_stop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   national_inv  lead_time  in_transit_qty  forecast_3_month  \\\n",
       "0          57.0        8.0             0.0              37.0   \n",
       "1          15.0        2.0             0.0               0.0   \n",
       "2          13.0       12.0             0.0               0.0   \n",
       "3           0.0        4.0             0.0               9.0   \n",
       "4           5.0        8.0            24.0              39.0   \n",
       "\n",
       "   forecast_6_month  forecast_9_month  sales_1_month  sales_3_month  \\\n",
       "0              84.0             131.0            0.0            0.0   \n",
       "1               0.0               0.0            0.0            0.0   \n",
       "2               0.0               0.0            0.0            0.0   \n",
       "3              10.0              10.0            0.0            0.0   \n",
       "4              67.0              95.0            5.0           25.0   \n",
       "\n",
       "   sales_6_month  sales_9_month  min_bank  potential_issue  pieces_past_due  \\\n",
       "0            0.0            0.0       0.0                0              0.0   \n",
       "1            1.0            3.0       0.0                0              0.0   \n",
       "2            0.0            0.0       1.0                0              0.0   \n",
       "3            0.0            0.0       0.0                0              0.0   \n",
       "4           50.0           71.0       0.0                0              0.0   \n",
       "\n",
       "   local_bo_qty  deck_risk  oe_constraint  ppap_risk  stop_auto_buy  rev_stop  \n",
       "0           0.0          0              0          0              1         0  \n",
       "1           0.0          0              0          0              1         0  \n",
       "2           0.0          1              0          0              1         0  \n",
       "3           0.0          1              0          0              1         0  \n",
       "4           0.0          0              0          0              1         0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: went_on_backorder, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize/standardize the data if required; otherwise ignore. You can perform this step inside the pipeline (if required). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=sampled_X\n",
    "y=sampled_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing Pipeline\n",
    "\n",
    "In this section, we design an operationalized machine learning pipeline, which includes:\n",
    "\n",
    "* Anomaly detection\n",
    "* Dimensionality Reduction\n",
    "* Train a classification model\n",
    "\n",
    "\n",
    "We are free to use any of the models that we learned in the past or we can use new models. Here is a pool of methods: \n",
    "\n",
    "### Pool of Anomaly Detection Methods (Discussed in M4)\n",
    "1. IsolationForest\n",
    "2. EllipticEnvelope\n",
    "3. LocalOutlierFactor\n",
    "4. OneClassSVM\n",
    "5. SGDOneClassSVM\n",
    "\n",
    "### Pool of Feature Selection Methods (Discussed in M3)\n",
    "\n",
    "1. VarianceThreshold\n",
    "1. SelectKBest with any scoring method (e.g, chi, f_classif, mutual_info_classif)\n",
    "1. SelectKPercentile\n",
    "3. SelectFpr, SelectFdr, or  SelectFwe\n",
    "1. GenericUnivariateSelect\n",
    "2. PCA\n",
    "3. Factor Analysis\n",
    "4. Variance Threshold\n",
    "5. RFE\n",
    "7. SelectFromModel\n",
    "\n",
    "\n",
    "### Classification Methods (Discussed in M1-M2\n",
    "1. Decision Tree\n",
    "2. Random Forest\n",
    "3. Logistic Regression\n",
    "4. Naive Bayes\n",
    "5. Linear SVC\n",
    "6. SVC with kernels\n",
    "7. KNeighborsClassifier\n",
    "8. GradientBoostingClassifier\n",
    "9. XGBClassifier\n",
    "10. LGBM Classifier\n",
    "\n",
    "\n",
    "\n",
    "It is difficult to fit an anomaly detection method in the sklearn pipeline without writing custom codes. For simplicity, we avoid fitting an anomaly detection method within a pipeline. So we can create the workflow in two steps. \n",
    "* Step I: fit an outlier with the training set\n",
    "* Step II: define a pipeline using a feature selection and a classification method. Then cross-validate this pipeline using the training data without outliers. \n",
    "* Note: if your smart sample is somewhat imbalanced, you might want to change the scoring method in GridSearchCV (see the [doc](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)).\n",
    "\n",
    "\n",
    "Once we fit the pipeline with gridsearch, we identify the best model and give an unbiased evaluation using the test set that we created in Part II. For unbiased evaluation we report confusion matrix, precision, recall, f1-score, accuracy, and other measures if you like. \n",
    "\n",
    "**Optional: Those who are interested in writing custom codes for adding an outlier detection method into the sklearn pipeline, please follow this discussion [thread](https://stackoverflow.com/questions/52346725/can-i-add-outlier-detection-and-removal-to-scikit-learn-pipeline).**\n",
    "\n",
    "\n",
    "**Note:** <span style='background:yellow'>We will be using Grid Search to find the optimal parameters of the pipelines.</span>\n",
    "\n",
    "You can add more notebook cells or import any Python modules as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, recall_score, fbeta_score\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from pprint import pprint\n",
    "from sklearn.neighbors import LocalOutlierFactor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your 1st pipeline \n",
    "  * Anomaly detection (Elliptic Envelope)\n",
    "  * Dimensionality reduction (SelectKBest)\n",
    "  * Model training/validation (Gradient Boosting Classifier)\n",
    "  \n",
    "Add cells as needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train=X_train\n",
    "y1_train=y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of outliers = 1514\n"
     ]
    }
   ],
   "source": [
    "# Add anomaly detection code  (Question #E201)\n",
    "# ----------------------------------\n",
    "envelope = EllipticEnvelope(support_fraction=1, contamination=0.1).fit(X1_train)\n",
    "\n",
    "# Create an boolean indexing array to pick up outliers\n",
    "outliers = envelope.predict(X1_train)==-1\n",
    "\n",
    "# Re-slice X,y into a cleaned dataset with outliers excluded\n",
    "X1_train__clean = X1_train[~outliers]\n",
    "y1_train_clean = y1_train[~outliers]\n",
    "\n",
    "print(f\"Num of outliers = {np.sum(outliers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import xgboost as xgb\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'ccp_alpha': 0.0,\n",
      " 'criterion': 'friedman_mse',\n",
      " 'init': None,\n",
      " 'learning_rate': 0.1,\n",
      " 'loss': 'deviance',\n",
      " 'max_depth': 3,\n",
      " 'max_features': None,\n",
      " 'max_leaf_nodes': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_impurity_split': None,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 100,\n",
      " 'n_iter_no_change': None,\n",
      " 'random_state': 42,\n",
      " 'subsample': 1.0,\n",
      " 'tol': 0.0001,\n",
      " 'validation_fraction': 0.1,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier(random_state = 42)\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(gbc.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add codes for feature selection and classification pipeline with grid search  (Question #E202)\n",
    "# ----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1xtra = Pipeline(steps=[('select', SelectKBest()),('gbc', GradientBoostingClassifier())])\n",
    "pipe1xtra.fit(X1_train, y1_train)\n",
    "\n",
    "param_gridxtra = {\n",
    "    'select__k': [8,10,15],\n",
    "    'gbc__max_features': ['auto','sqrt','log2'],\n",
    "    'gbc__learning_rate': [0.01,0.1,0.2,0.5],\n",
    "    'gbc__n_estimators': [50,100,500,1000]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('select', SelectKBest()),\n",
       "                                       ('gbc', GradientBoostingClassifier())]),\n",
       "             param_grid={'gbc__learning_rate': [0.01, 0.1, 0.2, 0.5],\n",
       "                         'gbc__max_features': ['auto', 'sqrt', 'log2'],\n",
       "                         'gbc__n_estimators': [50, 100, 500, 1000],\n",
       "                         'select__k': [8, 10, 15]})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gridxtra = GridSearchCV(pipe1xtra,param_gridxtra,cv=5)\n",
    "model_gridxtra.fit(X1_train,y1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('select', SelectKBest(k=15)),\n",
      "                ('gbc',\n",
      "                 GradientBoostingClassifier(max_features='auto',\n",
      "                                            n_estimators=500))])\n"
     ]
    }
   ],
   "source": [
    "print(model_gridxtra.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given an unbiased evaluation  (Question #E203)\n",
    "# ----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8846158532231725"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gridxtra.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred= model_gridxtra.predict(X1_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performance on entire training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predw= model_gridxtra.best_estimator_.predict(X1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.90      7566\n",
      "           1       0.90      0.92      0.91      7566\n",
      "\n",
      "    accuracy                           0.91     15132\n",
      "   macro avg       0.91      0.91      0.91     15132\n",
      "weighted avg       0.91      0.91      0.91     15132\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y1_train, train_predw)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6756  810]\n",
      " [ 623 6943]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y1_train,train_predw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.9053000264340471\n",
      "Recall Score:  0.9176579434311393\n",
      "F1 Score:  0.906456034989229\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Score: \", accuracy_score(y1_train,train_predw))\n",
    "print(\"Recall Score: \",recall_score(y1_train,train_predw))\n",
    "print(\"F1 Score: \",f1_score(y1_train,train_predw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performance on training data after outlier removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred= model_gridxtra.best_estimator_.predict(X1_train__clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.88      0.89      6728\n",
      "           1       0.89      0.91      0.90      6890\n",
      "\n",
      "    accuracy                           0.90     13618\n",
      "   macro avg       0.90      0.90      0.90     13618\n",
      "weighted avg       0.90      0.90      0.90     13618\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y1_train_clean, train_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5933  795]\n",
      " [ 611 6279]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y1_train_clean,train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.8967542957849904\n",
      "Recall Score:  0.9113207547169812\n",
      "F1 Score:  0.8993125179031797\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Score: \", accuracy_score(y1_train_clean,train_pred))\n",
    "print(\"Recall Score: \",recall_score(y1_train_clean,train_pred))\n",
    "print(\"F1 Score: \",f1_score(y1_train_clean,train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performance on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model_gridxtra.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.86      0.88      3727\n",
      "           1       0.87      0.90      0.88      3727\n",
      "\n",
      "    accuracy                           0.88      7454\n",
      "   macro avg       0.88      0.88      0.88      7454\n",
      "weighted avg       0.88      0.88      0.88      7454\n",
      "\n",
      "[[3205  522]\n",
      " [ 363 3364]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred)) \n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.8812718003756372\n",
      "Recall Score:  0.9026026294606923\n",
      "F1 Score:  0.8837514777354525\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Score: \", accuracy_score(y_test,x_pred))\n",
    "print(\"Recall Score: \",recall_score(y_test,x_pred))\n",
    "print(\"F1 Score: \",f1_score(y_test,x_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center>Record the optimal hyperparameters and performance resulting from this pipeline.</center>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Detail Hyperparameters and Results below  (Question #E204)\n",
    "# ---------------------------------------------\n",
    "\n",
    "Hyperparameters used for tuning:-\n",
    "SelectKBest:\n",
    "   k: An int or “all”, indicating the number of top features to select. The “all” option bypasses selection, for use in a parameter search\n",
    "   \n",
    "Gradient Boosting Classifier: \n",
    "   learning_rate: The learning rate shrinks the contribution of each tree by learning_rate. There is a trade-off between                         learning_rate and n_estimators. Values must be in the range [0.0, inf)\n",
    "   max_features: The number of features to consider when looking for the best split\n",
    "   n_estimators: The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number                  usually results in better performance. Values must be in the range [1, inf)\n",
    "\n",
    "\n",
    "Optimal hyperparameter values are:-\n",
    "Elliptic Envelope:\n",
    "   support_fraction = 1\n",
    "   contamination = 0.1\n",
    "\n",
    "SelectKBest:\n",
    "   k = 15\n",
    "   \n",
    "Gradient Boosting Classifier:\n",
    "   learning_rate = 0.5\n",
    "   max_features = 'auto'\n",
    "   n_estimators = 1000\n",
    "\n",
    "Best score from piepline: 0.8848132613921281\n",
    "\n",
    "\n",
    "Performance on training data:\n",
    "\n",
    "Accuracy: 0.91\n",
    "Recall: 0.92\n",
    "F1: 0.91\n",
    "\n",
    "Performance on training data after outliers:\n",
    "\n",
    "Accuracy: 0.90\n",
    "Recall: 0.91\n",
    "F1: 0.90\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"background: yellow;\">Commit your code!</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your 2nd pipeline\n",
    "  * Anomaly detection (Isolation Forest)\n",
    "  * Dimensionality reduction (SelectFromModel- LinearSVC)\n",
    "  * Model training/validation (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train=X_train\n",
    "y2_train=y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of outliers = 1211\n"
     ]
    }
   ],
   "source": [
    "# Add anomaly detection code  (Question #E205)\n",
    "# ----------------------------------\n",
    "# Construct IsolationForest \n",
    "iso_forest = IsolationForest(contamination=0.08).fit(X2_train, y2_train)\n",
    "\n",
    "# Get labels from classifier and cull outliers #P4006\n",
    "iso_outliers = iso_forest.predict(X2_train)==-1\n",
    "\n",
    "print(f\"Num of outliers = {np.sum(iso_outliers)}\")\n",
    "X2_iso = X2_train[~iso_outliers]\n",
    "y2_iso = y2_train[~iso_outliers]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'bootstrap': True,\n",
      " 'ccp_alpha': 0.0,\n",
      " 'class_weight': None,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': None,\n",
      " 'max_features': 'auto',\n",
      " 'max_leaf_nodes': None,\n",
      " 'max_samples': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_impurity_split': None,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 100,\n",
      " 'n_jobs': None,\n",
      " 'oob_score': False,\n",
      " 'random_state': 42,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state = 42)\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(rf.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Add codes for feature selection and classification pipeline with grid search  (Question #E206)\n",
    "# ----------------------------------\n",
    "pipe2 = Pipeline([\n",
    "  ('Lsvc', SelectFromModel(LinearSVC(penalty=\"l1\",dual=False))),\n",
    "  ('rf', RandomForestClassifier())\n",
    "])\n",
    "pipe2.fit(X2_train, y_train)\n",
    "\n",
    "param_grid2 = {\n",
    "    'rf__n_estimators': [200,600,1000,1400],\n",
    "    'rf__max_depth': [10,20,30,40,50],\n",
    "    'rf__max_features' : ['auto','sqrt']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_model2 = RandomizedSearchCV(pipe2,param_distributions=param_grid2,n_jobs=3,cv=8, n_iter=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('Lsvc',\n",
      "                 SelectFromModel(estimator=LinearSVC(dual=False,\n",
      "                                                     penalty='l1'))),\n",
      "                ('rf',\n",
      "                 RandomForestClassifier(max_depth=20, max_features='sqrt',\n",
      "                                        n_estimators=600))])\n"
     ]
    }
   ],
   "source": [
    "rand_model2.fit(X2_iso, y2_iso)\n",
    "\n",
    "# Check the best choosen params\n",
    "print(rand_model2.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9029513854502961"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_model2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([17.35040626,  5.74955598, 17.75475857, 12.36253399,  7.1678555 ,\n",
       "         7.29288924, 16.99081963,  7.34370896,  5.43790588, 16.99004331]),\n",
       " 'std_fit_time': array([0.33920847, 0.15687944, 0.42107386, 0.3626464 , 0.14952638,\n",
       "        0.12155904, 0.70699019, 0.33995919, 0.15053448, 0.57646339]),\n",
       " 'mean_score_time': array([0.67183232, 0.20813221, 0.66371647, 0.45752627, 0.27044716,\n",
       "        0.27280495, 0.63566518, 0.27168438, 0.19322512, 0.63367063]),\n",
       " 'std_score_time': array([0.01856905, 0.01038165, 0.01526353, 0.01663008, 0.00801413,\n",
       "        0.00835922, 0.01772929, 0.00624072, 0.00734648, 0.01841053]),\n",
       " 'param_rf__n_estimators': masked_array(data=[1400, 600, 1400, 1000, 600, 600, 1400, 600, 600, 1400],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_rf__max_features': masked_array(data=['sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'auto', 'sqrt',\n",
       "                    'sqrt', 'auto', 'auto'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_rf__max_depth': masked_array(data=[20, 10, 40, 50, 20, 30, 50, 40, 10, 30],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'rf__n_estimators': 1400,\n",
       "   'rf__max_features': 'sqrt',\n",
       "   'rf__max_depth': 20},\n",
       "  {'rf__n_estimators': 600, 'rf__max_features': 'sqrt', 'rf__max_depth': 10},\n",
       "  {'rf__n_estimators': 1400, 'rf__max_features': 'sqrt', 'rf__max_depth': 40},\n",
       "  {'rf__n_estimators': 1000, 'rf__max_features': 'sqrt', 'rf__max_depth': 50},\n",
       "  {'rf__n_estimators': 600, 'rf__max_features': 'sqrt', 'rf__max_depth': 20},\n",
       "  {'rf__n_estimators': 600, 'rf__max_features': 'auto', 'rf__max_depth': 30},\n",
       "  {'rf__n_estimators': 1400, 'rf__max_features': 'sqrt', 'rf__max_depth': 50},\n",
       "  {'rf__n_estimators': 600, 'rf__max_features': 'sqrt', 'rf__max_depth': 40},\n",
       "  {'rf__n_estimators': 600, 'rf__max_features': 'auto', 'rf__max_depth': 10},\n",
       "  {'rf__n_estimators': 1400, 'rf__max_features': 'auto', 'rf__max_depth': 30}],\n",
       " 'split0_test_score': array([0.91843768, 0.8914417 , 0.91671453, 0.91614015, 0.91671453,\n",
       "        0.9178633 , 0.91614015, 0.91671453, 0.894888  , 0.9178633 ]),\n",
       " 'split1_test_score': array([0.89425287, 0.86896552, 0.89425287, 0.89597701, 0.89310345,\n",
       "        0.89252874, 0.89137931, 0.89137931, 0.87068966, 0.89367816]),\n",
       " 'split2_test_score': array([0.90287356, 0.88218391, 0.90229885, 0.90114943, 0.90344828,\n",
       "        0.90057471, 0.90057471, 0.90114943, 0.8816092 , 0.90172414]),\n",
       " 'split3_test_score': array([0.89712644, 0.87643678, 0.89712644, 0.89712644, 0.89597701,\n",
       "        0.89770115, 0.8954023 , 0.89597701, 0.87643678, 0.89482759]),\n",
       " 'split4_test_score': array([0.90057471, 0.88735632, 0.89827586, 0.89942529, 0.90172414,\n",
       "        0.89885057, 0.90057471, 0.89770115, 0.88505747, 0.89712644]),\n",
       " 'split5_test_score': array([0.91321839, 0.8908046 , 0.91666667, 0.91494253, 0.91436782,\n",
       "        0.91436782, 0.91321839, 0.91091954, 0.89195402, 0.91321839]),\n",
       " 'split6_test_score': array([0.90344828, 0.89425287, 0.90229885, 0.90114943, 0.90229885,\n",
       "        0.90344828, 0.90114943, 0.90344828, 0.89482759, 0.90229885]),\n",
       " 'split7_test_score': array([0.89137931, 0.87988506, 0.89195402, 0.89310345, 0.89597701,\n",
       "        0.88965517, 0.8908046 , 0.88908046, 0.88333333, 0.89022989]),\n",
       " 'mean_test_score': array([0.90266391, 0.88391584, 0.90244851, 0.90237671, 0.90295139,\n",
       "        0.90187372, 0.90115545, 0.90079621, 0.88484951, 0.90137084]),\n",
       " 'std_test_score': array([0.00861673, 0.00807372, 0.00886537, 0.00801226, 0.00803277,\n",
       "        0.00921349, 0.00870734, 0.00881599, 0.00817714, 0.00906559]),\n",
       " 'rank_test_score': array([ 2, 10,  3,  4,  1,  5,  7,  8,  9,  6], dtype=int32)}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_model2.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performance on entire training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred2 = rand_model2.best_estimator_.predict(X2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96      7566\n",
      "           1       0.95      0.98      0.96      7566\n",
      "\n",
      "    accuracy                           0.96     15132\n",
      "   macro avg       0.96      0.96      0.96     15132\n",
      "weighted avg       0.96      0.96      0.96     15132\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y2_train, train_pred2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7179  387]\n",
      " [ 156 7410]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y2_train,train_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.9641157811260904\n",
      "Recall Score:  0.979381443298969\n",
      "F1 Score:  0.964655340753759\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Score: \", accuracy_score(y2_train,train_pred2))\n",
    "print(\"Recall Score: \",recall_score(y2_train,train_pred2))\n",
    "print(\"F1 Score: \",f1_score(y2_train,train_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performance on training data after outlier removal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred2iso = rand_model2.best_estimator_.predict(X2_iso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98      6980\n",
      "           1       0.98      0.99      0.98      6941\n",
      "\n",
      "    accuracy                           0.98     13921\n",
      "   macro avg       0.98      0.98      0.98     13921\n",
      "weighted avg       0.98      0.98      0.98     13921\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y2_iso, train_pred2iso)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6836  144]\n",
      " [  90 6851]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y2_iso,train_pred2iso))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.9831908627253789\n",
      "Recall Score:  0.9870335686500504\n",
      "F1 Score:  0.9832089552238806\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Score: \", accuracy_score(y2_iso,train_pred2iso))\n",
    "print(\"Recall Score: \",recall_score(y2_iso,train_pred2iso))\n",
    "print(\"F1 Score: \",f1_score(y2_iso,train_pred2iso))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performance on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.84      0.88      3727\n",
      "           1       0.85      0.93      0.89      3727\n",
      "\n",
      "    accuracy                           0.89      7454\n",
      "   macro avg       0.89      0.89      0.89      7454\n",
      "weighted avg       0.89      0.89      0.89      7454\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_y2 = rand_model2.predict(X_test)\n",
    "print(classification_report(y_test, predicted_y2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3126  601]\n",
      " [ 244 3483]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,predicted_y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.8866380466863429\n",
      "Recall Score:  0.934531795009391\n",
      "F1 Score:  0.8918192292920242\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Score: \", accuracy_score(y_test,predicted_y2))\n",
    "print(\"Recall Score: \",recall_score(y_test,predicted_y2))\n",
    "print(\"F1 Score: \",f1_score(y_test,predicted_y2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center>Record the optimal hyperparameters and performance resulting from this pipeline.</center>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Detail Hyperparameters and Results below  (Question #E208)\n",
    "# ---------------------------------------------\n",
    "\n",
    "Hyperparameters used for tuning:-\n",
    "SelectFromModel (LinearSVC):\n",
    "   penalty: The norm used in the penalization. It can be ‘l1’ or ‘l2’. The default value is 'l2’1. The ‘l1’ penalty leads to               sparse coef_ vectors, while the ‘l2’ penalty is the standard used in SVC\n",
    "   dual: Whether to solve the dual or primal optimization problem. Prefer dual=False when n_samples > n_features. The default            value is True\n",
    "   \n",
    "Random Forest Classifier: \n",
    "   max_depth: The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves                   contain less than min_samples_split samples.\n",
    "   n_estimators: The number of trees in the forest. The default value is 1001. A larger number usually results in better                        performance, but also increases the training time\n",
    "\n",
    "\n",
    "Optimal hyperparameter values are:-\n",
    "Isolation Forest:-\n",
    "   contamination = 0.08\n",
    "   \n",
    "SelectFromModel (LinearSVC):\n",
    "   penalty = 'l1'\n",
    "   dual = 'False'\n",
    "   \n",
    "Random Forest Classifier:\n",
    "   max_depth = 20\n",
    "   n_estimators = 1000\n",
    "   \n",
    "Best score from piepline: 0.9028793813173827\n",
    "\n",
    "Performance on training data:\n",
    "\n",
    "Accuracy: 0.96\n",
    "Recall: 0.98\n",
    "F1: 0.96\n",
    "\n",
    "Performance on training after outliers:\n",
    "\n",
    "Accuracy: 0.98\n",
    "Recall: 0.98\n",
    "F1: 0.98\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"background: yellow;\">Commit your code!</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your 3rd pipeline\n",
    "  * Anomaly detection (Local Outlier Factor)\n",
    "  * Dimensionality reduction (PCA)\n",
    "  * Model training/validation (SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3_train= X_train\n",
    "y3_train= y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add anomaly detection code  (Question #E209)\n",
    "# ----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof_labels = LocalOutlierFactor(n_neighbors=10).fit_predict(X3_train, y3_train)\n",
    "inliers = lof_labels == 1 # select inliers\n",
    "X3_clean = X3_train[inliers]\n",
    "y3_clean = y3_train[inliers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'C': 1.0,\n",
      " 'break_ties': False,\n",
      " 'cache_size': 200,\n",
      " 'class_weight': None,\n",
      " 'coef0': 0.0,\n",
      " 'decision_function_shape': 'ovr',\n",
      " 'degree': 3,\n",
      " 'gamma': 'scale',\n",
      " 'kernel': 'rbf',\n",
      " 'max_iter': -1,\n",
      " 'probability': False,\n",
      " 'random_state': 42,\n",
      " 'shrinking': True,\n",
      " 'tol': 0.001,\n",
      " 'verbose': False}\n"
     ]
    }
   ],
   "source": [
    "svc = SVC(random_state = 42)\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(svc.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pipe = Pipeline([\n",
    "    ('scale', MinMaxScaler()),\n",
    "    ('PCA', PCA()), \n",
    "    ('SVC', SVC()) \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'SVC__C': uniform(1000,100000),\n",
    "              'SVC__gamma': uniform(0.1,0.001), \n",
    "              'PCA__n_components': [10],\n",
    "              'SVC__kernel': ['rbf']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_model = RandomizedSearchCV(clf_pipe,param_distributions= param_grid, n_jobs=3,cv=5, n_iter=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scale', MinMaxScaler()), ('PCA', PCA(n_components=10)),\n",
      "                ('SVC', SVC(C=98495.71245065526, gamma=0.10055261454104605))])\n"
     ]
    }
   ],
   "source": [
    "rand_model.fit(X3_clean, y3_clean)\n",
    "\n",
    "# Check the best choosen params\n",
    "print(rand_model.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7844019368636539"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([12.12050505, 12.26125865, 12.18278093,  8.43253322, 12.1598495 ,\n",
       "        12.36184649]),\n",
       " 'std_fit_time': array([0.93559875, 0.82446408, 0.84118685, 0.28055503, 0.89801796,\n",
       "        0.91048539]),\n",
       " 'mean_score_time': array([1.01179118, 1.02409248, 0.98677077, 1.23256903, 0.97177997,\n",
       "        0.98433337]),\n",
       " 'std_score_time': array([0.01617578, 0.0266186 , 0.01985692, 0.01099956, 0.01438121,\n",
       "        0.02082859]),\n",
       " 'param_PCA__n_components': masked_array(data=[10, 10, 10, 10, 10, 10],\n",
       "              mask=[False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_SVC__C': masked_array(data=[68878.47400189278, 63272.143484310836,\n",
       "                    87725.74118358923, 5209.443108255428,\n",
       "                    98495.71245065526, 89434.60057516643],\n",
       "              mask=[False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_SVC__gamma': masked_array(data=[0.10042231518890962, 0.10047446402169648,\n",
       "                    0.1001812056513574, 0.10013998455324538,\n",
       "                    0.10055261454104605, 0.10020987882861068],\n",
       "              mask=[False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_SVC__kernel': masked_array(data=['rbf', 'rbf', 'rbf', 'rbf', 'rbf', 'rbf'],\n",
       "              mask=[False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'PCA__n_components': 10,\n",
       "   'SVC__C': 68878.47400189278,\n",
       "   'SVC__gamma': 0.10042231518890962,\n",
       "   'SVC__kernel': 'rbf'},\n",
       "  {'PCA__n_components': 10,\n",
       "   'SVC__C': 63272.143484310836,\n",
       "   'SVC__gamma': 0.10047446402169648,\n",
       "   'SVC__kernel': 'rbf'},\n",
       "  {'PCA__n_components': 10,\n",
       "   'SVC__C': 87725.74118358923,\n",
       "   'SVC__gamma': 0.1001812056513574,\n",
       "   'SVC__kernel': 'rbf'},\n",
       "  {'PCA__n_components': 10,\n",
       "   'SVC__C': 5209.443108255428,\n",
       "   'SVC__gamma': 0.10013998455324538,\n",
       "   'SVC__kernel': 'rbf'},\n",
       "  {'PCA__n_components': 10,\n",
       "   'SVC__C': 98495.71245065526,\n",
       "   'SVC__gamma': 0.10055261454104605,\n",
       "   'SVC__kernel': 'rbf'},\n",
       "  {'PCA__n_components': 10,\n",
       "   'SVC__C': 89434.60057516643,\n",
       "   'SVC__gamma': 0.10020987882861068,\n",
       "   'SVC__kernel': 'rbf'}],\n",
       " 'split0_test_score': array([0.77464789, 0.77464789, 0.78206079, 0.70830245, 0.78873239,\n",
       "        0.78502595]),\n",
       " 'split1_test_score': array([0.76760563, 0.76278725, 0.77242402, 0.70904374, 0.77835434,\n",
       "        0.78317272]),\n",
       " 'split2_test_score': array([0.76538176, 0.76278725, 0.77057079, 0.70348406, 0.77279466,\n",
       "        0.77279466]),\n",
       " 'split3_test_score': array([0.79050797, 0.78494624, 0.79532814, 0.7311828 , 0.80014831,\n",
       "        0.79940675]),\n",
       " 'split4_test_score': array([0.77233964, 0.76937338, 0.78309232, 0.69892473, 0.78197998,\n",
       "        0.7753059 ]),\n",
       " 'mean_test_score': array([0.77409658, 0.7709084 , 0.78069521, 0.71018755, 0.78440194,\n",
       "        0.78314119]),\n",
       " 'std_test_score': array([0.00884087, 0.00831032, 0.00886172, 0.01111261, 0.00942199,\n",
       "        0.00934413]),\n",
       " 'rank_test_score': array([4, 5, 3, 6, 1, 2], dtype=int32)}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given an unbiased evaluation  (Question #E211)\n",
    "# ----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performance on entire training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred3= rand_model.best_estimator_.predict(X3_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.60      0.72      7566\n",
      "           1       0.70      0.94      0.80      7566\n",
      "\n",
      "    accuracy                           0.77     15132\n",
      "   macro avg       0.80      0.77      0.76     15132\n",
      "weighted avg       0.80      0.77      0.76     15132\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y3_train, train_pred3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4516 3050]\n",
      " [ 482 7084]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y3_train,train_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.7665873645255089\n",
      "Recall Score:  0.936293946603225\n",
      "F1 Score:  0.8004519774011298\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Score: \", accuracy_score(y3_train,train_pred3))\n",
    "print(\"Recall Score: \",recall_score(y3_train,train_pred3))\n",
    "print(\"F1 Score: \",f1_score(y3_train,train_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performance on training after outlier removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred3c= rand_model.best_estimator_.predict(X3_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.61      0.73      6446\n",
      "           1       0.72      0.94      0.82      7042\n",
      "\n",
      "    accuracy                           0.78     13488\n",
      "   macro avg       0.82      0.77      0.77     13488\n",
      "weighted avg       0.81      0.78      0.78     13488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y3_clean, train_pred3c)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3916 2530]\n",
      " [ 405 6637]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y3_clean,train_pred3c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.7823991696322657\n",
      "Recall Score:  0.9424879295654643\n",
      "F1 Score:  0.8189277561848355\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Score: \", accuracy_score(y3_clean,train_pred3c))\n",
    "print(\"Recall Score: \",recall_score(y3_clean,train_pred3c))\n",
    "print(\"F1 Score: \",f1_score(y3_clean,train_pred3c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performance on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.58      0.71      3727\n",
      "           1       0.69      0.93      0.79      3727\n",
      "\n",
      "    accuracy                           0.76      7454\n",
      "   macro avg       0.79      0.76      0.75      7454\n",
      "weighted avg       0.79      0.76      0.75      7454\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_y3 = rand_model.predict(X_test)\n",
    "print(classification_report(y_test, predicted_y3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2164 1563]\n",
      " [ 243 3484]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,predicted_y3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.7577139790716394\n",
      "Recall Score:  0.9348001073249262\n",
      "F1 Score:  0.7941645771597902\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Score: \", accuracy_score(y_test,predicted_y3))\n",
    "print(\"Recall Score: \",recall_score(y_test,predicted_y3))\n",
    "print(\"F1 Score: \",f1_score(y_test,predicted_y3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center>Record the optimal hyperparameters and performance resulting from this pipeline.</center>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Detail Hyperparameters and Results below  (Question #E212)\n",
    "# ---------------------------------------------\n",
    "\n",
    "Hyperparameters used for tuning:-\n",
    "PCA:\n",
    "   n_components: The number of principal components to compute.\n",
    "   \n",
    "SVC: \n",
    "   C: Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive.         The penalty is a squared l2 penalty.\n",
    "   gamma:  Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’. If gamma=‘scale’ (default) is passed then it uses 1 /                      (n_features * X.var ()) as value of gamma, if ‘auto’, uses 1 / n_features.\n",
    "   kernel: Specifies the kernel type to be used in the algorithm. It can be ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’              or a callable.\n",
    "\n",
    "Optimal hyperparameter values are:-\n",
    "LOF:\n",
    "   n_neighbors= 10\n",
    "\n",
    "PCA:\n",
    "   n_components: 10\n",
    "   \n",
    "SVC: \n",
    "   C: 96472.12\n",
    "   gamma: 0.1\n",
    "   kernel: 'rbf' (decided by trial and error)\n",
    "   \n",
    "Best score from piepline: 0.7790639628415066\n",
    "\n",
    "Performance on training data:\n",
    "\n",
    "Accuracy: 0.77\n",
    "Recall: 0.94\n",
    "F1: 0.80\n",
    "\n",
    "Performance on training data after outliers:\n",
    "\n",
    "Accuracy: 0.78\n",
    "Recall: 0.94\n",
    "F1: 0.82\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare these three pipelines and discuss your findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.86      0.88      3727\n",
      "           1       0.87      0.90      0.88      3727\n",
      "\n",
      "    accuracy                           0.88      7454\n",
      "   macro avg       0.88      0.88      0.88      7454\n",
      "weighted avg       0.88      0.88      0.88      7454\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred=model_gridxtra.best_estimator_.predict(X_test)\n",
    "print(classification_report(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3205  522]\n",
      " [ 363 3364]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.8812718003756372\n",
      "Recall Score:  0.9026026294606923\n",
      "F1 Score:  0.8837514777354525\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Score: \", accuracy_score(y_test,x_pred))\n",
    "print(\"Recall Score: \",recall_score(y_test,x_pred))\n",
    "print(\"F1 Score: \",f1_score(y_test,x_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.84      0.88      3727\n",
      "           1       0.85      0.93      0.89      3727\n",
      "\n",
      "    accuracy                           0.89      7454\n",
      "   macro avg       0.89      0.89      0.89      7454\n",
      "weighted avg       0.89      0.89      0.89      7454\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_y2 = rand_model2.predict(X_test)\n",
    "print(classification_report(y_test, predicted_y2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3126  601]\n",
      " [ 244 3483]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,predicted_y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.8866380466863429\n",
      "Recall Score:  0.934531795009391\n",
      "F1 Score:  0.8918192292920242\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Score: \", accuracy_score(y_test,predicted_y2))\n",
    "print(\"Recall Score: \",recall_score(y_test,predicted_y2))\n",
    "print(\"F1 Score: \",f1_score(y_test,predicted_y2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.58      0.71      3727\n",
      "           1       0.69      0.93      0.79      3727\n",
      "\n",
      "    accuracy                           0.76      7454\n",
      "   macro avg       0.79      0.76      0.75      7454\n",
      "weighted avg       0.79      0.76      0.75      7454\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_y3 = rand_model.best_estimator_.predict(X_test)\n",
    "print(classification_report(y_test, predicted_y3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2164 1563]\n",
      " [ 243 3484]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,predicted_y3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.7577139790716394\n",
      "Recall Score:  0.9348001073249262\n",
      "F1 Score:  0.7941645771597902\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Score: \", accuracy_score(y_test,predicted_y3))\n",
    "print(\"Recall Score: \",recall_score(y_test,predicted_y3))\n",
    "print(\"F1 Score: \",f1_score(y_test,predicted_y3))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Write your analysis in this cell (Question #E213)\n",
    "# ----------------------------------\n",
    "\n",
    "Pipeline 1: EllipticEnvelope (Outlier Detection) + SelectKBest (Feature Selection) + Gradient Boosting Classifier (Classifier)\n",
    "Accuracy: 0.88\n",
    "Recall: 0.90\n",
    "F1 score: 0.88\n",
    "\n",
    "Pipeline 2: Isolation Forest (Outlier Detection) + SelectFromModel(LinearSVC) (Feature Selection) + Random Forest (Classifier)\n",
    "Accuracy: 0.90\n",
    "Recall: 0.92\n",
    "F1 score: 0.90\n",
    "\n",
    "Pipeline 3: Local Outlier Factor (Outlier Detection) + MinMaxScalar (Scaling) + PCA (Feature Selection) + SVC (Classifier)\n",
    "Accuracy: 0.76\n",
    "Recall: 0.93\n",
    "F1 score: 0.80\n",
    "\n",
    "Out the three pipelines the ones with ensemble classifiers gave the significantly better results. Tree based approach appears to be suitable for this dataset. \n",
    "\n",
    "In all pipelines, 10-15 features were used. For the tree based algorithms - gradient boosting and random forest- 1000 was identified as the best number of estimators.\n",
    "\n",
    "The second pipeline works best in terms of accuracy and having higher accuracy value for the target class. I used random search for the second pipeline rather than grid search.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"background: yellow;\">Commit your code!</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle the required pipeline/models for Part III."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IsolationForest(contamination=0.08)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iso_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('Lsvc',\n",
       "                 SelectFromModel(estimator=LinearSVC(dual=False,\n",
       "                                                     penalty='l1'))),\n",
       "                ('rf',\n",
       "                 RandomForestClassifier(max_depth=20, max_features='sqrt',\n",
       "                                        n_estimators=600))])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_model2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best_model.joblib']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_pipe= rand_model2.best_estimator_\n",
    "\n",
    "joblib.dump(best_pipe,'best_model.joblib')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iso_forest.joblib']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(iso_forest,'iso_forest.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have made a few commits so far of this project.  \n",
    "**Definitely make a commit of the notebook now!**  \n",
    "Comment should be: `Final Project, Checkpoint - Pipelines done`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook!\n",
    "## Then `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
