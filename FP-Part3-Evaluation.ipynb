{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Unbiased Evaluation using a New Test Set\n",
    "\n",
    "In this part, we are given a new test set (`/dsa/data/all_datasets/back_order/Kaggle_Test_Dataset_v2.csv`). We can now take advantage of the entire smart sample that we created in Part I. \n",
    "\n",
    "* Retrain a pipeline using the optimal parameters that the pipeline learned. We don't need to repeat GridSearch here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, recall_score, fbeta_score\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from pprint import pprint\n",
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load smart sample and the best pipeline from Part II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_X, sampled_y= joblib.load('sampled_data.pkl')\n",
    "\n",
    "bestmodel = joblib.load('best_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_forest= joblib.load('iso_forest.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('Lsvc',\n",
       "                 SelectFromModel(estimator=LinearSVC(dual=False,\n",
       "                                                     penalty='l1'))),\n",
       "                ('rf',\n",
       "                 RandomForestClassifier(max_depth=20, max_features='sqrt',\n",
       "                                        n_estimators=600))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IsolationForest(contamination=0.08)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iso_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  Retrain a pipeline using the full sampled training data set\n",
    "\n",
    "Use the full sampled training data set to train the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=sampled_X\n",
    "y=sampled_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IsolationForest(contamination=0.08)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add code below this comment  (Question #E301)\n",
    "# ----------------------------------\n",
    "iso_forest.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of outliers = 1807\n"
     ]
    }
   ],
   "source": [
    "iso_outliers = iso_forest.predict(X)==-1\n",
    "\n",
    "print(f\"Num of outliers = {np.sum(iso_outliers)}\")\n",
    "X_iso = X[~iso_outliers]\n",
    "y_iso = y[~iso_outliers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm=bestmodel.fit(X_iso, y_iso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('Lsvc',\n",
       "                 SelectFromModel(estimator=LinearSVC(dual=False,\n",
       "                                                     penalty='l1'))),\n",
       "                ('rf',\n",
       "                 RandomForestClassifier(max_depth=20, max_features='sqrt',\n",
       "                                        n_estimators=600))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = bestmodel.predict(X_iso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98     10398\n",
      "           1       0.97      0.99      0.98     10381\n",
      "\n",
      "    accuracy                           0.98     20779\n",
      "   macro avg       0.98      0.98      0.98     20779\n",
      "weighted avg       0.98      0.98      0.98     20779\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_iso, y_pred_train)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10111   287]\n",
      " [  137 10244]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_iso,y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.9795947831945715\n",
      "Recall Score:  0.9868028128311338\n",
      "F1 Score:  0.9797245600612089\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Score: \", accuracy_score(y_iso,y_pred_train))\n",
    "print(\"Recall Score: \",recall_score(y_iso,y_pred_train))\n",
    "print(\"F1 Score: \",f1_score(y_iso,y_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the trained model with the pickle library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code below this comment  \n",
    "# -----------------------------\n",
    "import pickle\n",
    "pickle.dump(bm,open('finalmodel.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Load the Testing Data and evaluate your model\n",
    "\n",
    " * `/dsa/data/all_datasets/back_order/Kaggle_Test_Dataset_v2.csv`\n",
    " \n",
    "* We need to preprocess this test data (**follow** the steps similar to Part I)\n",
    "* **If you have fitted any normalizer/standardizer in Part 2, then we have to transform this test data using the fitted normalizer/standardizer!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sku</th>\n",
       "      <td>3290188</td>\n",
       "      <td>3453759</td>\n",
       "      <td>3512840</td>\n",
       "      <td>3515426</td>\n",
       "      <td>3520962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>national_inv</th>\n",
       "      <td>135.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lead_time</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in_transit_qty</th>\n",
       "      <td>67.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forecast_3_month</th>\n",
       "      <td>144.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>288.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forecast_6_month</th>\n",
       "      <td>324.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>288.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forecast_9_month</th>\n",
       "      <td>504.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>288.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sales_1_month</th>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sales_3_month</th>\n",
       "      <td>95.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sales_6_month</th>\n",
       "      <td>194.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sales_9_month</th>\n",
       "      <td>247.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_bank</th>\n",
       "      <td>51.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>potential_issue</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pieces_past_due</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perf_6_month_avg</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perf_12_month_avg</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.95</td>\n",
       "      <td>-99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>local_bo_qty</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deck_risk</th>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oe_constraint</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ppap_risk</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stop_auto_buy</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rev_stop</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>went_on_backorder</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0        1        2        3        4\n",
       "sku                3290188  3453759  3512840  3515426  3520962\n",
       "national_inv         135.0     38.0     27.0     -4.0     61.0\n",
       "lead_time              2.0      2.0      8.0      8.0      NaN\n",
       "in_transit_qty        67.0      0.0      0.0      0.0      0.0\n",
       "forecast_3_month     144.0      0.0      0.0    288.0      0.0\n",
       "forecast_6_month     324.0      0.0      0.0    288.0      0.0\n",
       "forecast_9_month     504.0      0.0      0.0    288.0      0.0\n",
       "sales_1_month         27.0      0.0      0.0      1.0      1.0\n",
       "sales_3_month         95.0      0.0      1.0    153.0      4.0\n",
       "sales_6_month        194.0      0.0      5.0    231.0      7.0\n",
       "sales_9_month        247.0      0.0     13.0    398.0     11.0\n",
       "min_bank              51.0      1.0      1.0     13.0      2.0\n",
       "potential_issue         No       No       No       No       No\n",
       "pieces_past_due        0.0      0.0      0.0      0.0      0.0\n",
       "perf_6_month_avg      0.82     0.63     0.99      1.0    -99.0\n",
       "perf_12_month_avg     0.68     0.55     0.99     0.95    -99.0\n",
       "local_bo_qty           0.0      0.0      0.0      4.0      0.0\n",
       "deck_risk               No      Yes       No       No      Yes\n",
       "oe_constraint           No       No       No       No       No\n",
       "ppap_risk               No       No       No      Yes       No\n",
       "stop_auto_buy          Yes      Yes      Yes      Yes      Yes\n",
       "rev_stop                No       No       No       No       No\n",
       "went_on_backorder       No       No       No      Yes       No"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess the given test set  (Question #E302)\n",
    "# ----------------------------------\n",
    "\n",
    "# Dataset location\n",
    "DATASET = '/dsa/data/all_datasets/back_order/Kaggle_Test_Dataset_v2.csv'\n",
    "assert os.path.exists(DATASET)\n",
    "\n",
    "# Load and shuffle\n",
    "dataset = pd.read_csv(DATASET).sample(frac = 1).reset_index(drop=True)\n",
    "\n",
    "dataset.head().transpose()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(242076, 23)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 242076 entries, 0 to 242075\n",
      "Data columns (total 23 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   sku                242076 non-null  object \n",
      " 1   national_inv       242075 non-null  float64\n",
      " 2   lead_time          227351 non-null  float64\n",
      " 3   in_transit_qty     242075 non-null  float64\n",
      " 4   forecast_3_month   242075 non-null  float64\n",
      " 5   forecast_6_month   242075 non-null  float64\n",
      " 6   forecast_9_month   242075 non-null  float64\n",
      " 7   sales_1_month      242075 non-null  float64\n",
      " 8   sales_3_month      242075 non-null  float64\n",
      " 9   sales_6_month      242075 non-null  float64\n",
      " 10  sales_9_month      242075 non-null  float64\n",
      " 11  min_bank           242075 non-null  float64\n",
      " 12  potential_issue    242075 non-null  object \n",
      " 13  pieces_past_due    242075 non-null  float64\n",
      " 14  perf_6_month_avg   242075 non-null  float64\n",
      " 15  perf_12_month_avg  242075 non-null  float64\n",
      " 16  local_bo_qty       242075 non-null  float64\n",
      " 17  deck_risk          242075 non-null  object \n",
      " 18  oe_constraint      242075 non-null  object \n",
      " 19  ppap_risk          242075 non-null  object \n",
      " 20  stop_auto_buy      242075 non-null  object \n",
      " 21  rev_stop           242075 non-null  object \n",
      " 22  went_on_backorder  242075 non-null  object \n",
      "dtypes: float64(15), object(8)\n",
      "memory usage: 42.5+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop(['sku'], axis=1,inplace=True)   # unique identifier is not required. It is a mix of integer and string values\n",
    "# Source performance for past 6 months and 12 months seems irrelevant. The values range from -0.99 and 1.0 which is ambigous\n",
    "dataset.drop(['perf_6_month_avg'], axis=1, inplace=True,)  \n",
    "dataset.drop(['perf_12_month_avg'], axis=1, inplace=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['national_inv', 'lead_time', 'in_transit_qty', 'forecast_3_month',\n",
       "       'forecast_6_month', 'forecast_9_month', 'sales_1_month',\n",
       "       'sales_3_month', 'sales_6_month', 'sales_9_month', 'min_bank',\n",
       "       'potential_issue', 'pieces_past_due', 'local_bo_qty', 'deck_risk',\n",
       "       'oe_constraint', 'ppap_risk', 'stop_auto_buy', 'rev_stop',\n",
       "       'went_on_backorder'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['potential_issue', 'deck_risk', 'oe_constraint', 'ppap_risk', 'stop_auto_buy', 'rev_stop', 'went_on_backorder']\n",
      "potential_issue ['No' 'Yes' nan]\n",
      "deck_risk ['No' 'Yes' nan]\n",
      "oe_constraint ['No' 'Yes' nan]\n",
      "ppap_risk ['No' 'Yes' nan]\n",
      "stop_auto_buy ['Yes' 'No' nan]\n",
      "rev_stop ['No' 'Yes' nan]\n",
      "went_on_backorder ['No' 'Yes' nan]\n"
     ]
    }
   ],
   "source": [
    "# All the column names of these yes/no columns\n",
    "yes_no_columns = list(filter(lambda i: dataset[i].dtype!=np.float64, dataset.columns))\n",
    "print(yes_no_columns)\n",
    "\n",
    "# Add code below this comment  (Question #E102)\n",
    "# ----------------------------------\n",
    "print('potential_issue',dataset['potential_issue'].unique())\n",
    "print('deck_risk',dataset['deck_risk'].unique())\n",
    "print('oe_constraint',dataset['oe_constraint'].unique())\n",
    "print('ppap_risk',dataset['ppap_risk'].unique())\n",
    "print('stop_auto_buy',dataset['stop_auto_buy'].unique())\n",
    "print('rev_stop',dataset['rev_stop'].unique())\n",
    "print('went_on_backorder',dataset['went_on_backorder'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling missing values of potential_issue with No\n",
      "Filling missing values of deck_risk with No\n",
      "Filling missing values of oe_constraint with No\n",
      "Filling missing values of ppap_risk with No\n",
      "Filling missing values of stop_auto_buy with Yes\n",
      "Filling missing values of rev_stop with No\n",
      "Filling missing values of went_on_backorder with No\n"
     ]
    }
   ],
   "source": [
    "for column_name in yes_no_columns:\n",
    "    mode = dataset[column_name].apply(str).mode()[0]\n",
    "    print('Filling missing values of {} with {}'.format(column_name, mode))\n",
    "    dataset[column_name].fillna(mode, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now predict and evaluate with the preprocessed test set. It would be interesting to see the performance with and without outliers removal from the test set. We can report confusion matrix, precision, recall, f1-score, accuracy, and other measures (if any). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for colname in yes_no_columns:\n",
    "    dataset[colname].replace(('Yes','No'),(1,0), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "potential_issue [0 1]\n",
      "deck_risk [0 1]\n",
      "oe_constraint [0 1]\n",
      "ppap_risk [0 1]\n",
      "stop_auto_buy [1 0]\n",
      "rev_stop [0 1]\n",
      "went_on_backorder [0 1]\n"
     ]
    }
   ],
   "source": [
    "print('potential_issue',dataset['potential_issue'].unique())\n",
    "print('deck_risk',dataset['deck_risk'].unique())\n",
    "print('oe_constraint',dataset['oe_constraint'].unique())\n",
    "print('ppap_risk',dataset['ppap_risk'].unique())\n",
    "print('stop_auto_buy',dataset['stop_auto_buy'].unique())\n",
    "print('rev_stop',dataset['rev_stop'].unique())\n",
    "print('went_on_backorder',dataset['went_on_backorder'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backorder ratio: 2688 / 242076 = 0.01110395082536063\n"
     ]
    }
   ],
   "source": [
    "num_backorder = np.sum(dataset['went_on_backorder']==1)\n",
    "print('backorder ratio:', num_backorder, '/', len(dataset), '=', num_backorder / len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "national_inv             1\n",
       "lead_time            14725\n",
       "in_transit_qty           1\n",
       "forecast_3_month         1\n",
       "forecast_6_month         1\n",
       "forecast_9_month         1\n",
       "sales_1_month            1\n",
       "sales_3_month            1\n",
       "sales_6_month            1\n",
       "sales_9_month            1\n",
       "min_bank                 1\n",
       "potential_issue          0\n",
       "pieces_past_due          1\n",
       "local_bo_qty             1\n",
       "deck_risk                0\n",
       "oe_constraint            0\n",
       "ppap_risk                0\n",
       "stop_auto_buy            0\n",
       "rev_stop                 0\n",
       "went_on_backorder        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_median = dataset['lead_time'].median()\n",
    "dataset['lead_time'].fillna(lt_median,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping only one row with all NA values\n",
    "dataset=dataset.dropna(subset=['national_inv','in_transit_qty','forecast_3_month','forecast_6_month','forecast_9_month',\n",
    "                              'sales_1_month','sales_3_month','sales_6_month','sales_9_month','min_bank','pieces_past_due',\n",
    "                              'local_bo_qty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(242075, 20)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickled_model = pickle.load(open('finalmodel.pkl', 'rb'))\n",
    "#y_predicted= pickled_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = dataset.iloc[:, 0:-1]\n",
    "y_df = dataset.iloc[:, -1].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #E303)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "y_predicted = pickled_model.predict(X_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94    239387\n",
      "           1       0.08      0.81      0.14      2688\n",
      "\n",
      "    accuracy                           0.89    242075\n",
      "   macro avg       0.54      0.85      0.54    242075\n",
      "weighted avg       0.99      0.89      0.93    242075\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_df, y_predicted)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[213205  26182]\n",
      " [   505   2183]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_df,y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.8897573066198492\n",
      "Recall Score:  0.8121279761904762\n",
      "F1 Score:  0.14059833188419799\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Score: \", accuracy_score(y_df,y_predicted))\n",
    "print(\"Recall Score: \",recall_score(y_df,y_predicted))\n",
    "print(\"F1 Score: \",f1_score(y_df,y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Write a summary of your processing and an analysis of the model performance  \n",
    "# (Question #E304)\n",
    "# ----------------------------------\n",
    "\n",
    "Preprocessing the unseen data:\n",
    "\n",
    "  Basic EDA was done on the test data, similar to Part 1. \n",
    "  Columns that were evidently irrelevant were removed. 'sku' was the unique identifier of the dataset which is not required.     'perf_6_mont_avg' and 'perf_12_month_avg' seemed irrelevant and ambigous. The rperformance values ranged from -0.99 to 1.0     which was ambigous to interpret.\n",
    "\n",
    "  Missing values were handled by using the median value for the feature 'lead_time' with over 14,000 missing values. There was   one row which had misiing values in almost all the columns, that row was eliminated from the dataset.\n",
    "\n",
    "Model Performance:\n",
    "\n",
    "  The model used was the best model from Part 2. [Isolation Forest + SelectFromModel(LinearSVC) + Random Forest Classifier]\n",
    "  \n",
    "  Hyperparamters: \n",
    "    1) IsolationForest(contamination=0.08)\n",
    "    2) Pipeline(steps=[('Lsvc',\n",
    "                 SelectFromModel(estimator=LinearSVC(dual=False,\n",
    "                                                     penalty='l1'))),\n",
    "                ('rf',\n",
    "                 RandomForestClassifier(max_depth=20, max_features='sqrt',\n",
    "                                        n_estimators=600))])\n",
    "                                 \n",
    "  The accuracy of the model on training data (sampled from Part 1) is 98%, with recall of 99% and F1 score of 98%.\n",
    "  The model has high precision, recall and F1 score for both the classes on the sampled data.\n",
    "  \n",
    "  On the unseen data, the overall model accuracy obtained is 89%, with recall of 81% and F1 score of 14.1%.\n",
    "  \n",
    "  The model does extremely well in predicting the majority class with precision of 100% and recall value of 89%.\n",
    "  The model has a high recall value of 81% for the minority class but poor precision of 8%.\n",
    "  \n",
    "Minority Class: (Went to backorder- yes)\n",
    "  \n",
    "  For every 100 instances for the minority class, 81 were predicted correctly. So we see that the model does well in predicting   the minority class (when the product goes for backorder). However, it is predicting much more samples as the                   minority class (went to backorder- yes), than the actual number of samples that did go to backorder. That is, there is higher   number of false positives. For every 100 predicted as positive (went to backorder- yes), only 8 were true, is actually went     to backorder. (went to backorder- yes).\n",
    "  \n",
    "  The model is sensitive to backordered items, having a high true positive rate. [Sensitivity is the porbability of a positive   result conditioned on the individual/sample truly being positive]. The items taht should go to backorder were correctly         predicted.\n",
    "  However, it has lower specificity (True Negative Rate) which is the probability of a negative result conditioned on the         individual/sample truly being negative]. This means that there items that didnt actually go to backorder were predicted to go   into backorder.\n",
    "  \n",
    "  There is usually a trade-ff between sensitivity and specificity, such that higher sensitivities will mean lower specificities   (and vice versa) which is what we can observe with above model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflect\n",
    "\n",
    "Imagine you are data scientist that has been tasked with developing a system to save your \n",
    "company money by predicting and preventing back orders of parts in the supply chain.\n",
    "\n",
    "Write a **brief summary** for \"management\" that details your findings, \n",
    "your level of certainty and trust in the models, \n",
    "and recommendations for operationalizing these models for the business."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Write your answer here:  \n",
    "# (Question #E305)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "[Assumptions: (1) The business sells items that are critical for the function or sustainence of fast-paced industry (eg,         medical equipments  (2) The business has competitiors and wants to stay ahead of the curve to meet the market's high demand.]\n",
    "\n",
    "\n",
    "                                                      Problem:\n",
    "\n",
    "Understanding the need of the business to resolve the issue of product availability in situations of high demand, it is crucial to higlight the problems that come with backorderering items. If the company consistently sees items in backorder, \n",
    " - it could be taken as a signal that the company's operations are too lean\n",
    " - it could also mean that the company is losing out on business by not providing the products demanded by its customers\n",
    " - if a customer sees products on backorder—and notices this frequently—they may decide to cancel orders, forcing the company      to issue refunds and readjust their books.\n",
    " - if the expected wait time until the product becomes available is long, customers may look elsewhere for a substitute, that      is, loss of customer to competition\n",
    " - it may require additional resources in managing pre-orders or clients that are waiting for their product.\n",
    " - it could lead to eventual loss of market share as customers become frustrated with the lack of product              availability\n",
    " - due to improper inventory management, increase in overhead costs such as logistics and public communication\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                            Model and proposed solution:\n",
    "\n",
    "Understanding above problems and consequences of the lack of product availability, a machine learning model is proposed to help predict if an item is needed to be stocked up in the inventory, ie, go for backorder or not. This is an efficient way to guage the demand and the current state of supply chain operations for the company.\n",
    "\n",
    "The model provides an accuracy of 89% which means that it acurately predicts 89% of the times, whether an item must go to backorder or not. \n",
    "\n",
    "However, it is grasped that the issue is to better the availability of the product that is expected to meet a high demand or is currently insufficient in amount at the company inventory. Hence, the model proposed is one that is sensitive to such items. \n",
    "The recall score for the cases where the items are subjected to backorder is 81% which means that out of 100 items, 81 are rightly identified which are required to be restocked/backordered. This is a good estimator for backorder items and there is a low chance of missing those items which need to be restocked. This will save the company from running out of supply for such items. Meeting the customer demand at the promised time would build better customer trust and inturn customer loyalty to the company. This will reduce the overhead costs and unnecessary resource planning for order cancellations, refunds, etc. With higher customer satisfaction, the market share could increase with increased order numbers. This could directly affect revenue generation positively, but also give the company the scope to increase their prices with respect to competition as they can promise delivery in time.\n",
    "\n",
    "This model does,however, have a trade-off in precision, of predicting the items which are wrongly classified as backordered when they did not require so. The precision rate of the model is 8% which means that out of 100 items predicted to be backordered, only 8 actually went to backorder.This could translate to increased supply to the inventory for products that may not have as significant demand as projected. This could come with a cost. The products would require more marketing and resources to manage selling before expiry of itmes and in some cases also produce more storage and transportation cost. This can be avoided by judging the current market trends or knowledge of the product history. This may also be corrected by estimating the following orders considering inflation of inventory. \n",
    "\n",
    "The ultimate decision of using the model defintely depends on the consequence and gravity of missing products that should require to be backordered, or made available but also depend on the tolerance of having to inflate the inventory. However, this model can be trusted to not miss the items with the genuine need and demand. \n",
    "\n",
    "The best way to make use of this model would be to flag the items predicted as positive (for backorder), cautiously order the items based on order history and other knowledge while constantly monitoring the flagged products of changing needs of customers and market trends.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook!\n",
    "## Then `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
